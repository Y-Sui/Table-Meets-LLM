# evaluate table benchmark
#python evaluate_benchmark.py --task_group table --log_file_path table_benchmarks_20230116_log --model text003 text002
python evaluate_benchmark.py --task_group form --log_file_path form_benchmarks_20230115_log --model text003 text002